# ğŸ–¼ï¸ Attention-Guided-Vision-Language-Modeling-for-Image-and-Video-Captioning

This repository presents an advanced image captioning framework that generates detailed and context-aware captions by combining transformer-based encoders and attention-enhanced decoders. It leverages visual and relational cues in the image to improve caption quality and generalization.

---

## ğŸ“Œ Overview

This model integrates:

- **BLIP Visual Encoder**: Extracts high-level semantic features from the image
- **GPT-2 Language Decoder**: Generates fluent and descriptive text
- **Scene Attention**: Captures holistic scene-level context
- **Object Attention**: Focuses on detailed object-specific features
- **Graph Interaction Attention**: Models relationships between objects using similarity-based graph reasoning

The combination improves both visual understanding and linguistic coherence.

---

## ğŸ›  Features

- Transformer-based vision-language architecture
- Fine-grained and contextual attention modules
- Supports MSCOCO, NoCaps, and custom datasets
- Evaluation using CIDEr, BLEU, METEOR, ROUGE-L, and SPICE
- Modular design for easy extension and experimentation

---

## ğŸ“ Directory Structure
â”œâ”€â”€ data/ # Processed datasets and annotations â”œâ”€â”€ models/ # Model definitions â”‚ â”œâ”€â”€ blip_encoder.py # Visual encoder â”‚ â”œâ”€â”€ gpt2_decoder.py # Language decoder â”‚ â”œâ”€â”€ attention_modules.py # Scene/Object/Graph Attention â”œâ”€â”€ utils/ # Helper functions and evaluation scripts â”œâ”€â”€ train.py # Model training script â”œâ”€â”€ evaluate.py # Model evaluation script â”œâ”€â”€ configs/ # YAML config files â”œâ”€â”€ checkpoints/ # Saved model weights â””â”€â”€ README.md # Project documentation

